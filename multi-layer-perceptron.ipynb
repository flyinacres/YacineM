{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-06T02:46:38.810520Z","iopub.execute_input":"2024-03-06T02:46:38.811297Z","iopub.status.idle":"2024-03-06T02:46:40.051263Z","shell.execute_reply.started":"2024-03-06T02:46:38.811263Z","shell.execute_reply":"2024-03-06T02:46:40.050024Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"From original: The goal of this notebook is to showcase how to code a multilayer perceptron in Python from scratch. It makes use of the Perceptron algorithm we developed in the perceptron jupyter notebook with modification to use backpropagation!\n\nWe have two main class:\n\nNeuron: Used to model one neuron, most of the computation happens there\nMultiLayerPerceptron: Used to model the full neural network. Here we only support fully connected neural network.\n\nIn this notebook we will code it in such a way that we can have a variable architecture.\nThe one limitation here is that we still only have one output that is possible 1 or 0. We still didn't got around to have arbitrary amount of output (getting there).","metadata":{}},{"cell_type":"code","source":"import random\nimport math\n\nclass Neuron():\n    '''\n        A conceptual Neuron that can be trained using a \n        fit and predict methodology, without any library\n    '''\n    \n    def __init__(self, position_in_layer, is_output_neuron=False):\n        self.weights = []\n        self.inputs = []\n        self.output = None\n        \n        # This is used for the backpropagation update\n        self.updated_weights = []\n        # This is used to know how to update the weights\n        self.is_output_neuron = is_output_neuron\n        # This delta is used for the update at the backpropagation\n        self.delta = None\n        # This is used for the backpropagation update\n        self.position_in_layer = position_in_layer \n        \n    def attach_to_output(self, neurons):\n        '''\n            Helper function to store the reference of the other neurons\n            To this particular neuron (used for backpropagation)\n        '''\n        \n        self.output_neurons = neurons\n    \n    def sigmoid(self, x):\n        '''\n            simple sigmoid function (logistic) used for the activation\n        '''\n        return 1 / (1 + math.exp(-x))\n    \n    def init_weights(self, num_input):\n        '''\n            This is used to setup the weights when we know how many inputs there is for\n            a given neuron\n        '''\n        \n        # Randomly initalize the weights\n        # With numpy you could do this...\n        # self.weights = np.random.rand(num_input+1)\n        # Without numpy, you  need to do this...\n        for i in range(num_input+1):\n           self.weights.append(random.uniform(0,1))\n        \n    def predict(self, row):\n        '''\n            Given a row of data it will predict what the output should be for\n            this given neuron. We can have many input, but only one output for a neuron\n        '''\n        \n        # Reset the inputs\n        self.inputs = []\n        \n        # We iterate over the weights and the features in the given row\n        activation = 0\n        for weight, feature in zip(self.weights, row):\n            self.inputs.append(feature)\n            activation = activation + weight*feature\n            \n        \n        self.output = self.sigmoid(activation)\n        return self.output\n    \n        \n            \n    def update_neuron(self):\n        '''\n            Will update a given neuron weights by replacing the current weights\n            with those used during the backpropagation. This need to be done at the end of the\n            backpropagation\n        '''\n        \n        self.weights = []\n        for new_weight in self.updated_weights:\n            self.weights.append(new_weight)\n    \n    def calculate_update(self, learning_rate, target):\n        '''\n            This function will calculate the updated weights for this neuron. It will first calculate\n            the right delta (depending if this neuron is a ouput or a hidden neuron), then it will\n            calculate the right updated_weights. It will not overwrite the weights yet as they are needed\n            for other update in the backpropagation algorithm.\n        '''\n        \n        if self.is_output_neuron:\n            # Calculate the delta for the output\n            self.delta = (self.output - target)*self.output*(1-self.output)\n        else:\n            # Calculate the delta\n            delta_sum = 0\n            # this is to know which weights this neuron is contributing in the output layer\n            cur_weight_index = self.position_in_layer \n            for output_neuron in self.output_neurons:\n                delta_sum = delta_sum + (output_neuron.delta * output_neuron.weights[cur_weight_index])\n\n            # Update this neuron delta\n            self.delta = delta_sum*self.output*(1-self.output)\n            \n            \n        # Reset the update weights\n        self.updated_weights = []\n        \n        # Iterate over each weight and update them\n        for cur_weight, cur_input in zip(self.weights, self.inputs):\n            gradient = self.delta*cur_input\n            new_weight = cur_weight - learning_rate*gradient\n            self.updated_weights.append(new_weight)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T02:53:18.440026Z","iopub.execute_input":"2024-03-06T02:53:18.441106Z","iopub.status.idle":"2024-03-06T02:53:18.454373Z","shell.execute_reply.started":"2024-03-06T02:53:18.441069Z","shell.execute_reply":"2024-03-06T02:53:18.453499Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class Layer():\n    '''\n        Layer is modelizing a layer in the fully-connected-feedforward neural network architecture.\n        It will play the role of connecting everything together inside and will be doing the backpropagation \n        update.\n    '''\n    \n    def __init__(self, num_neuron, is_output_layer = False):\n        \n        # Will create that much neurons in this layer\n        self.is_output_layer = is_output_layer\n        self.neurons = []\n        for i in range(num_neuron):\n            # Create neuron\n            neuron = Neuron(i,  is_output_neuron=is_output_layer)\n            self.neurons.append(neuron)\n    \n    def attach(self, layer):\n        '''\n            This function attach the neurons from this layer to another one\n            This is needed for the backpropagation algorithm\n        '''\n        # Iterate over the neurons in the current layer and attach \n        # them to the next layer\n        for in_neuron in self.neurons:\n            in_neuron.attach_to_output(layer.neurons)\n            \n    def init_layer(self, num_input):\n        '''\n            This will initialize the weights of each neuron in the layer.\n            By giving the right num_input it will spawn the right number of weights\n        '''\n        \n        # Iterate over each of the neuron and initialize\n        # the weights that connect with the previous layer\n        for neuron in self.neurons:\n            neuron.init_weights(num_input)\n    \n    def predict(self, row):\n        '''\n            This will calcualte the activations for the full layer given the row of data \n            streaming in.\n        '''\n        row.append(1) # need to add the bias\n        activations = [neuron.predict(row) for neuron in self.neurons]\n        return activations","metadata":{"execution":{"iopub.status.busy":"2024-03-06T02:46:58.733845Z","iopub.execute_input":"2024-03-06T02:46:58.734651Z","iopub.status.idle":"2024-03-06T02:46:58.742339Z","shell.execute_reply.started":"2024-03-06T02:46:58.734617Z","shell.execute_reply":"2024-03-06T02:46:58.741191Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class MultiLayerPerceptron():\n    '''\n        We will be creating the multi-layer perceptron with only two layer:\n        an input layer, a perceptrons layer and a one neuron output layer which does binary classification\n    '''\n    def __init__(self, learning_rate = 0.01, num_iteration = 100):\n        \n        # Layers\n        self.layers = []\n                \n        # Training parameters\n        self.learning_rate = learning_rate\n        self.num_iteration = num_iteration\n        \n        \n    def add_output_layer(self, num_neuron):\n        '''\n            This helper function will create a new output layer and add it to the architecture\n        '''\n        self.layers.insert(0, Layer(num_neuron, is_output_layer = True))\n    \n    def add_hidden_layer(self, num_neuron):\n        '''\n            This helper function will create a new hidden layer, add it to the architecture\n            and finally attach it to the front of the architecture\n        '''\n        # Create an hidden layer\n        hidden_layer = Layer(num_neuron)\n        # Attach the last added layer to this new layer\n        hidden_layer.attach(self.layers[0])\n        # Add this layers to the architecture\n        self.layers.insert(0, hidden_layer)\n        \n    def update_layers(self, target):\n        '''\n            Will update all the layers by calculating the updated weights and then updating \n            the weights all at once when the new weights are found.\n        '''\n        # Iterate over each of the layer in reverse order\n        # to calculate the updated weights\n        for layer in reversed(self.layers):\n                           \n            # Calculate update the hidden layer\n            for neuron in layer.neurons:\n                neuron.calculate_update(self.learning_rate, target)  \n        \n        # Iterate over each of the layer in normal order\n        # to update the weights\n        for layer in self.layers:\n            for neuron in layer.neurons:\n                neuron.update_neuron()\n    \n    def fit(self, X, y):\n        '''\n            Main training function of the neural network algorithm. This will make use of backpropagation.\n            It will use stochastic gradient descent by selecting one row at random from the dataset and \n            use predict to calculate the error. The error will then be backpropagated and new weights calculated.\n            Once all the new weights are calculated, the whole network weights will be updated\n        '''\n        num_row = len(X)\n        num_feature = len(X[0]) # Here we assume that we have a rectangular matrix\n        \n        # Init the weights throughout each of the layer\n        self.layers[0].init_layer(num_feature)\n        \n        for i in range(1, len(self.layers)):\n            num_input = len(self.layers[i-1].neurons)\n            self.layers[i].init_layer(num_input)\n\n        # Launch the training algorithm\n        for i in range(self.num_iteration):\n            \n            # Stochastic Gradient Descent\n            r_i = random.randint(0,num_row-1)\n            row = X[r_i] # take the random sample from the dataset\n            yhat = self.predict(row)\n            target = y[r_i]\n            \n            # Update the layers using backpropagation   \n            self.update_layers(target)\n            \n            # At every 100 iteration we calculate the error\n            # on the whole training set\n            if i % 1000 == 0:\n                total_error = 0\n                for r_i in range(num_row):\n                    row = X[r_i]\n                    yhat = self.predict(row)\n                    error = (y[r_i] - yhat)\n                    total_error = total_error + error**2\n                mean_error = total_error/num_row\n                print(f\"Iteration {i} with error = {mean_error}\")\n        \n    \n    def predict(self, row):\n        '''\n            Prediction function that will take a row of input and give back the output\n            of the whole neural network.\n        '''\n        \n        # Gather all the activation in the hidden layer\n        \n        activations = self.layers[0].predict(row)\n        for i in range(1, len(self.layers)):\n            activations = self.layers[i].predict(activations)\n\n        outputs = []\n        for activation in activations:                        \n            # Decide if we output a 1 or 0\n            if activation >= 0.5:\n                outputs.append(1.0)\n            else:\n                outputs.append(0.0)\n                           \n        # We currently have only One output allowed\n        return outputs[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-06T02:47:02.261782Z","iopub.execute_input":"2024-03-06T02:47:02.262191Z","iopub.status.idle":"2024-03-06T02:47:02.277268Z","shell.execute_reply.started":"2024-03-06T02:47:02.262160Z","shell.execute_reply":"2024-03-06T02:47:02.275950Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# XOR function (one or the other but not both)\nX = [[0,0], [0,1], [1,0], [1,1]]\ny = [0, 1, 1, 0]\n\n# Init the parameters for the network\nclf = MultiLayerPerceptron(learning_rate = 0.1, num_iteration = 100000)\n# Create the architecture backward\nclf.add_output_layer(num_neuron = 1)\nclf.add_hidden_layer(num_neuron = 3)\nclf.add_hidden_layer(num_neuron = 2)\n# Train the network\nclf.fit(X,y)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T02:52:09.094474Z","iopub.execute_input":"2024-03-06T02:52:09.094873Z","iopub.status.idle":"2024-03-06T02:52:12.940822Z","shell.execute_reply.started":"2024-03-06T02:52:09.094843Z","shell.execute_reply":"2024-03-06T02:52:12.939704Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Iteration 0 with error = 0.5\nIteration 1000 with error = 0.5\nIteration 2000 with error = 0.5\nIteration 3000 with error = 0.5\nIteration 4000 with error = 0.5\nIteration 5000 with error = 0.5\nIteration 6000 with error = 0.5\nIteration 7000 with error = 0.5\nIteration 8000 with error = 0.5\nIteration 9000 with error = 0.5\nIteration 10000 with error = 0.5\nIteration 11000 with error = 0.5\nIteration 12000 with error = 0.5\nIteration 13000 with error = 0.5\nIteration 14000 with error = 0.5\nIteration 15000 with error = 0.5\nIteration 16000 with error = 0.5\nIteration 17000 with error = 0.5\nIteration 18000 with error = 0.5\nIteration 19000 with error = 0.5\nIteration 20000 with error = 0.5\nIteration 21000 with error = 0.5\nIteration 22000 with error = 0.5\nIteration 23000 with error = 0.5\nIteration 24000 with error = 0.5\nIteration 25000 with error = 0.5\nIteration 26000 with error = 0.5\nIteration 27000 with error = 0.5\nIteration 28000 with error = 0.5\nIteration 29000 with error = 0.5\nIteration 30000 with error = 0.5\nIteration 31000 with error = 0.5\nIteration 32000 with error = 0.5\nIteration 33000 with error = 0.5\nIteration 34000 with error = 0.5\nIteration 35000 with error = 0.5\nIteration 36000 with error = 0.5\nIteration 37000 with error = 0.5\nIteration 38000 with error = 0.5\nIteration 39000 with error = 0.5\nIteration 40000 with error = 0.5\nIteration 41000 with error = 0.5\nIteration 42000 with error = 0.5\nIteration 43000 with error = 0.5\nIteration 44000 with error = 0.5\nIteration 45000 with error = 0.5\nIteration 46000 with error = 0.5\nIteration 47000 with error = 0.5\nIteration 48000 with error = 0.5\nIteration 49000 with error = 0.5\nIteration 50000 with error = 0.5\nIteration 51000 with error = 0.5\nIteration 52000 with error = 0.5\nIteration 53000 with error = 0.5\nIteration 54000 with error = 0.5\nIteration 55000 with error = 0.5\nIteration 56000 with error = 0.5\nIteration 57000 with error = 0.5\nIteration 58000 with error = 0.5\nIteration 59000 with error = 0.5\nIteration 60000 with error = 0.25\nIteration 61000 with error = 0.5\nIteration 62000 with error = 0.5\nIteration 63000 with error = 0.5\nIteration 64000 with error = 0.5\nIteration 65000 with error = 0.5\nIteration 66000 with error = 0.5\nIteration 67000 with error = 0.5\nIteration 68000 with error = 0.5\nIteration 69000 with error = 0.5\nIteration 70000 with error = 0.5\nIteration 71000 with error = 0.5\nIteration 72000 with error = 0.5\nIteration 73000 with error = 0.5\nIteration 74000 with error = 0.5\nIteration 75000 with error = 0.5\nIteration 76000 with error = 0.5\nIteration 77000 with error = 0.5\nIteration 78000 with error = 0.5\nIteration 79000 with error = 0.5\nIteration 80000 with error = 0.5\nIteration 81000 with error = 0.5\nIteration 82000 with error = 0.5\nIteration 83000 with error = 0.5\nIteration 84000 with error = 0.5\nIteration 85000 with error = 0.5\nIteration 86000 with error = 0.25\nIteration 87000 with error = 0.5\nIteration 88000 with error = 0.25\nIteration 89000 with error = 0.25\nIteration 90000 with error = 0.5\nIteration 91000 with error = 0.25\nIteration 92000 with error = 0.25\nIteration 93000 with error = 0.25\nIteration 94000 with error = 0.25\nIteration 95000 with error = 0.25\nIteration 96000 with error = 0.25\nIteration 97000 with error = 0.25\nIteration 98000 with error = 0.0\nIteration 99000 with error = 0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Expected 0.0, got: \",clf.predict([0,0]))\nprint(\"Expected 1.0, got: \",clf.predict([0,1]))\nprint(\"Expected 1.0, got: \",clf.predict([1,0]))\nprint(\"Expected 0.0, got: \",clf.predict([1,1]))","metadata":{"execution":{"iopub.status.busy":"2024-03-06T02:52:18.503435Z","iopub.execute_input":"2024-03-06T02:52:18.503790Z","iopub.status.idle":"2024-03-06T02:52:18.509826Z","shell.execute_reply.started":"2024-03-06T02:52:18.503766Z","shell.execute_reply":"2024-03-06T02:52:18.508753Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Expected 0.0, got:  0.0\nExpected 1.0, got:  1.0\nExpected 1.0, got:  1.0\nExpected 0.0, got:  0.0\n","output_type":"stream"}]}]}