{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-06T02:46:38.810520Z","iopub.execute_input":"2024-03-06T02:46:38.811297Z","iopub.status.idle":"2024-03-06T02:46:40.051263Z","shell.execute_reply.started":"2024-03-06T02:46:38.811263Z","shell.execute_reply":"2024-03-06T02:46:40.050024Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"From original: The goal of this notebook is to showcase how to code a multilayer perceptron in Python from scratch. It makes use of the Perceptron algorithm we developped in the perceptron jupyter notebook with modification to use backpropagation!\n\nWe have two main class:\n\nNeuron: Is used to model one neuron, most of the computation happens there\nMultiLayerPerceptron: Is used to model the full neural network. Here we only support fully connected neural network.\nIn this notebook we will code it in such a way that we can have a variable architecture.\nThe one limitation here is that we still only have one output that is possible 1 or 0. We still didn't got around to have arbitrary amount of output (getting there).","metadata":{}},{"cell_type":"code","source":"import random\nimport math\n\nclass Neuron():\n    '''\n        A conceptual Neuron hat can be trained using a \n        fit and predict methodology, without any library\n    '''\n    \n    def __init__(self, position_in_layer, is_output_neuron=False):\n        self.weights = []\n        self.inputs = []\n        self.output = None\n        \n        # This is used for the backpropagation update\n        self.updated_weights = []\n        # This is used to know how to update the weights\n        self.is_output_neuron = is_output_neuron\n        # This delta is used for the update at the backpropagation\n        self.delta = None\n        # This is used for the backpropagation update\n        self.position_in_layer = position_in_layer \n        \n    def attach_to_output(self, neurons):\n        '''\n            Helper function to store the reference of the other neurons\n            To this particular neuron (used for backpropagation)\n        '''\n        \n        self.output_neurons = neurons\n    \n    def sigmoid(self, x):\n        '''\n            simple sigmoid function (logistic) used for the activation\n        '''\n        return 1 / (1 + math.exp(-x))\n    \n    def init_weights(self, num_input):\n        '''\n            This is used to setup the weights when we know how many inputs there is for\n            a given neuron\n        '''\n        \n        # Randomly initalize the weights\n        for i in range(num_input+1):\n            self.weights.append(random.uniform(0,1))\n        \n    def predict(self, row):\n        '''\n            Given a row of data it will predict what the output should be for\n            this given neuron. We can have many input, but only one output for a neuron\n        '''\n        \n        # Reset the inputs\n        self.inputs = []\n        \n        # We iterate over the weights and the features in the given row\n        activation = 0\n        for weight, feature in zip(self.weights, row):\n            self.inputs.append(feature)\n            activation = activation + weight*feature\n            \n        \n        self.output = self.sigmoid(activation)\n        return self.output\n    \n        \n            \n    def update_neuron(self):\n        '''\n            Will update a given neuron weights by replacing the current weights\n            with those used during the backpropagation. This need to be done at the end of the\n            backpropagation\n        '''\n        \n        self.weights = []\n        for new_weight in self.updated_weights:\n            self.weights.append(new_weight)\n    \n    def calculate_update(self, learning_rate, target):\n        '''\n            This function will calculate the updated weights for this neuron. It will first calculate\n            the right delta (depending if this neuron is a ouput or a hidden neuron), then it will\n            calculate the right updated_weights. It will not overwrite the weights yet as they are needed\n            for other update in the backpropagation algorithm.\n        '''\n        \n        if self.is_output_neuron:\n            # Calculate the delta for the output\n            self.delta = (self.output - target)*self.output*(1-self.output)\n        else:\n            # Calculate the delta\n            delta_sum = 0\n            # this is to know which weights this neuron is contributing in the output layer\n            cur_weight_index = self.position_in_layer \n            for output_neuron in self.output_neurons:\n                delta_sum = delta_sum + (output_neuron.delta * output_neuron.weights[cur_weight_index])\n\n            # Update this neuron delta\n            self.delta = delta_sum*self.output*(1-self.output)\n            \n            \n        # Reset the update weights\n        self.updated_weights = []\n        \n        # Iterate over each weight and update them\n        for cur_weight, cur_input in zip(self.weights, self.inputs):\n            gradient = self.delta*cur_input\n            new_weight = cur_weight - learning_rate*gradient\n            self.updated_weights.append(new_weight)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T02:46:53.564352Z","iopub.execute_input":"2024-03-06T02:46:53.564735Z","iopub.status.idle":"2024-03-06T02:46:53.579686Z","shell.execute_reply.started":"2024-03-06T02:46:53.564708Z","shell.execute_reply":"2024-03-06T02:46:53.578790Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Layer():\n    '''\n        Layer is modelizing a layer in the fully-connected-feedforward neural network architecture.\n        It will play the role of connecting everything together inside and will be doing the backpropagation \n        update.\n    '''\n    \n    def __init__(self, num_neuron, is_output_layer = False):\n        \n        # Will create that much neurons in this layer\n        self.is_output_layer = is_output_layer\n        self.neurons = []\n        for i in range(num_neuron):\n            # Create neuron\n            neuron = Neuron(i,  is_output_neuron=is_output_layer)\n            self.neurons.append(neuron)\n    \n    def attach(self, layer):\n        '''\n            This function attach the neurons from this layer to another one\n            This is needed for the backpropagation algorithm\n        '''\n        # Iterate over the neurons in the current layer and attach \n        # them to the next layer\n        for in_neuron in self.neurons:\n            in_neuron.attach_to_output(layer.neurons)\n            \n    def init_layer(self, num_input):\n        '''\n            This will initialize the weights of each neuron in the layer.\n            By giving the right num_input it will spawn the right number of weights\n        '''\n        \n        # Iterate over each of the neuron and initialize\n        # the weights that connect with the previous layer\n        for neuron in self.neurons:\n            neuron.init_weights(num_input)\n    \n    def predict(self, row):\n        '''\n            This will calcualte the activations for the full layer given the row of data \n            streaming in.\n        '''\n        row.append(1) # need to add the bias\n        activations = [neuron.predict(row) for neuron in self.neurons]\n        return activations","metadata":{"execution":{"iopub.status.busy":"2024-03-06T02:46:58.733845Z","iopub.execute_input":"2024-03-06T02:46:58.734651Z","iopub.status.idle":"2024-03-06T02:46:58.742339Z","shell.execute_reply.started":"2024-03-06T02:46:58.734617Z","shell.execute_reply":"2024-03-06T02:46:58.741191Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class MultiLayerPerceptron():\n    '''\n        We will be creating the multi-layer perceptron with only two layer:\n        an input layer, a perceptrons layer and a one neuron output layer which does binary classification\n    '''\n    def __init__(self, learning_rate = 0.01, num_iteration = 100):\n        \n        # Layers\n        self.layers = []\n                \n        # Training parameters\n        self.learning_rate = learning_rate\n        self.num_iteration = num_iteration\n        \n        \n    def add_output_layer(self, num_neuron):\n        '''\n            This helper function will create a new output layer and add it to the architecture\n        '''\n        self.layers.insert(0, Layer(num_neuron, is_output_layer = True))\n    \n    def add_hidden_layer(self, num_neuron):\n        '''\n            This helper function will create a new hidden layer, add it to the architecture\n            and finally attach it to the front of the architecture\n        '''\n        # Create an hidden layer\n        hidden_layer = Layer(num_neuron)\n        # Attach the last added layer to this new layer\n        hidden_layer.attach(self.layers[0])\n        # Add this layers to the architecture\n        self.layers.insert(0, hidden_layer)\n        \n    def update_layers(self, target):\n        '''\n            Will update all the layers by calculating the updated weights and then updating \n            the weights all at once when the new weights are found.\n        '''\n        # Iterate over each of the layer in reverse order\n        # to calculate the updated weights\n        for layer in reversed(self.layers):\n                           \n            # Calculate update the hidden layer\n            for neuron in layer.neurons:\n                neuron.calculate_update(self.learning_rate, target)  \n        \n        # Iterate over each of the layer in normal order\n        # to update the weights\n        for layer in self.layers:\n            for neuron in layer.neurons:\n                neuron.update_neuron()\n    \n    def fit(self, X, y):\n        '''\n            Main training function of the neural network algorithm. This will make use of backpropagation.\n            It will use stochastic gradient descent by selecting one row at random from the dataset and \n            use predict to calculate the error. The error will then be backpropagated and new weights calculated.\n            Once all the new weights are calculated, the whole network weights will be updated\n        '''\n        num_row = len(X)\n        num_feature = len(X[0]) # Here we assume that we have a rectangular matrix\n        \n        # Init the weights throughout each of the layer\n        self.layers[0].init_layer(num_feature)\n        \n        for i in range(1, len(self.layers)):\n            num_input = len(self.layers[i-1].neurons)\n            self.layers[i].init_layer(num_input)\n\n        # Launch the training algorithm\n        for i in range(self.num_iteration):\n            \n            # Stochastic Gradient Descent\n            r_i = random.randint(0,num_row-1)\n            row = X[r_i] # take the random sample from the dataset\n            yhat = self.predict(row)\n            target = y[r_i]\n            \n            # Update the layers using backpropagation   \n            self.update_layers(target)\n            \n            # At every 100 iteration we calculate the error\n            # on the whole training set\n            if i % 1000 == 0:\n                total_error = 0\n                for r_i in range(num_row):\n                    row = X[r_i]\n                    yhat = self.predict(row)\n                    error = (y[r_i] - yhat)\n                    total_error = total_error + error**2\n                mean_error = total_error/num_row\n                print(f\"Iteration {i} with error = {mean_error}\")\n        \n    \n    def predict(self, row):\n        '''\n            Prediction function that will take a row of input and give back the output\n            of the whole neural network.\n        '''\n        \n        # Gather all the activation in the hidden layer\n        \n        activations = self.layers[0].predict(row)\n        for i in range(1, len(self.layers)):\n            activations = self.layers[i].predict(activations)\n\n        outputs = []\n        for activation in activations:                        \n            # Decide if we output a 1 or 0\n            if activation >= 0.5:\n                outputs.append(1.0)\n            else:\n                outputs.append(0.0)\n                           \n        # We currently have only One output allowed\n        return outputs[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-06T02:47:02.261782Z","iopub.execute_input":"2024-03-06T02:47:02.262191Z","iopub.status.idle":"2024-03-06T02:47:02.277268Z","shell.execute_reply.started":"2024-03-06T02:47:02.262160Z","shell.execute_reply":"2024-03-06T02:47:02.275950Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":" XOR function (one or the other but not both)\nX = [[0,0], [0,1], [1,0], [1,1]]\ny = [0, 1, 1, 0]\n\n# Init the parameters for the network\nclf = MultiLayerPerceptron(learning_rate = 0.1, num_iteration = 100000)\n# Create the architecture backward\nclf.add_output_layer(num_neuron = 1)\nclf.add_hidden_layer(num_neuron = 3)\nclf.add_hidden_layer(num_neuron = 2)\n# Train the network\nclf.fit(X,y)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T02:47:06.043711Z","iopub.execute_input":"2024-03-06T02:47:06.044159Z","iopub.status.idle":"2024-03-06T02:47:06.052970Z","shell.execute_reply.started":"2024-03-06T02:47:06.044127Z","shell.execute_reply":"2024-03-06T02:47:06.051442Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    XOR function (one or the other but not both)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (2222836629.py, line 1)","output_type":"error"}]}]}