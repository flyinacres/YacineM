{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":3966491,"datasetId":2354152}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-05T04:23:22.787249Z","iopub.execute_input":"2024-03-05T04:23:22.787658Z","iopub.status.idle":"2024-03-05T04:23:23.269395Z","shell.execute_reply.started":"2024-03-05T04:23:22.787620Z","shell.execute_reply":"2024-03-05T04:23:23.267857Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"TODO: Go through this in more detail to see how the two implementations vary, and why they give different results.","metadata":{}},{"cell_type":"markdown","source":"Note: Figured out where the input is stored by default...  Note the directory structure.\n\nSince I used a dataset in Kaggle with the same info but slightly different format I had to edit some details to get this all to work.","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/iris-dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-05T04:33:59.294699Z","iopub.execute_input":"2024-03-05T04:33:59.295154Z","iopub.status.idle":"2024-03-05T04:34:00.416311Z","shell.execute_reply.started":"2024-03-05T04:33:59.295116Z","shell.execute_reply":"2024-03-05T04:34:00.414828Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"iris.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!head /kaggle/input/iris-dataset/iris.csv","metadata":{"execution":{"iopub.status.busy":"2024-03-05T04:34:22.362483Z","iopub.execute_input":"2024-03-05T04:34:22.362889Z","iopub.status.idle":"2024-03-05T04:34:23.468774Z","shell.execute_reply.started":"2024-03-05T04:34:22.362856Z","shell.execute_reply":"2024-03-05T04:34:23.467532Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"sepal_length,sepal_width,petal_length,petal_width,species\n5.1,3.5,1.4,0.2,setosa\n4.9,3.0,1.4,0.2,setosa\n4.7,3.2,1.3,0.2,setosa\n4.6,3.1,1.5,0.2,setosa\n5.0,3.6,1.4,0.2,setosa\n5.4,3.9,1.7,0.4,setosa\n4.6,3.4,1.4,0.3,setosa\n5.0,3.4,1.5,0.2,setosa\n4.4,2.9,1.4,0.2,setosa\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport random\n\n\n# Perceptron with numpy + pandas\n# The guy is great, will definetly give a shoutout\nclass Perceptron():\n    '''\n        Perceptron Learning Algorithm that can be train using a \n        fit and predict methodology with numpy\n    '''\n    \n    def __init__(self):\n        self.weights = []\n        \n    def fit(self, X, y, learning_rate = 0.01, num_iteration = 100):\n        \n        (num_row, num_feature) = X.shape\n        \n        # Randomly initalize the weights\n        self.weights = np.random.rand(num_feature+1) \n\n        # Launch the training algorithm\n        for i in range(num_iteration):\n            \n            # Stochastic Gradient Descent\n            r_i = random.randint(0,num_row-1)\n            row = X[r_i,:] # take the random sample from the dataset\n            yhat = self.predict(row)\n            error = (y[r_i] - yhat) # estimate of the gradient\n            self.weights[0] = self.weights[0] + learning_rate*error*1 # first weight one is the bias\n\n            # Update all parameters after bias\n            for f_i in range(num_feature):\n                self.weights[f_i] = self.weights[f_i] + learning_rate*error*row[f_i]\n                \n            if i % 100 == 0:\n                total_error = 0\n                for r_i in range(num_row):\n                    row = X[r_i,:]\n                    yhat = self.predict(row)\n                    error = (y[r_i] - yhat)\n                    total_error = total_error + error**2\n                mean_error = total_error/num_row\n                print(f\"Iteration {i} with error = {mean_error}\")\n        \n    def predict(self, row):\n            \n        # The activation start with the bias at weights == 0\n        activation = self.weights[0]\n        \n        # We iterate over the weights and the features in the given row\n        for weight, feature in zip(self.weights[1:], row):\n            activation = activation + weight*feature\n            \n        # Heaviside Step Function Activation\n        if activation >= 0.0:\n            return 1.0\n        return 0.0","metadata":{"execution":{"iopub.status.busy":"2024-03-05T04:31:52.646031Z","iopub.execute_input":"2024-03-05T04:31:52.646502Z","iopub.status.idle":"2024-03-05T04:31:52.661816Z","shell.execute_reply.started":"2024-03-05T04:31:52.646457Z","shell.execute_reply":"2024-03-05T04:31:52.660476Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Data sets\ndf = pd.read_csv('/kaggle/input/iris-dataset/iris.csv')\n\ndf.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2024-03-05T04:39:53.878278Z","iopub.execute_input":"2024-03-05T04:39:53.878679Z","iopub.status.idle":"2024-03-05T04:39:53.914135Z","shell.execute_reply.started":"2024-03-05T04:39:53.878650Z","shell.execute_reply":"2024-03-05T04:39:53.912863Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"        sepal_length  sepal_width  petal_length  petal_width species\ncount     150.000000   150.000000    150.000000   150.000000     150\nunique           NaN          NaN           NaN          NaN       3\ntop              NaN          NaN           NaN          NaN  setosa\nfreq             NaN          NaN           NaN          NaN      50\nmean        5.843333     3.054000      3.758667     1.198667     NaN\nstd         0.828066     0.433594      1.764420     0.763161     NaN\nmin         4.300000     2.000000      1.000000     0.100000     NaN\n25%         5.100000     2.800000      1.600000     0.300000     NaN\n50%         5.800000     3.000000      4.350000     1.300000     NaN\n75%         6.400000     3.300000      5.100000     1.800000     NaN\nmax         7.900000     4.400000      6.900000     2.500000     NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal_length</th>\n      <th>sepal_width</th>\n      <th>petal_length</th>\n      <th>petal_width</th>\n      <th>species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.843333</td>\n      <td>3.054000</td>\n      <td>3.758667</td>\n      <td>1.198667</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.828066</td>\n      <td>0.433594</td>\n      <td>1.764420</td>\n      <td>0.763161</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4.300000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>0.100000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.100000</td>\n      <td>2.800000</td>\n      <td>1.600000</td>\n      <td>0.300000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.800000</td>\n      <td>3.000000</td>\n      <td>4.350000</td>\n      <td>1.300000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.400000</td>\n      <td>3.300000</td>\n      <td>5.100000</td>\n      <td>1.800000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.900000</td>\n      <td>4.400000</td>\n      <td>6.900000</td>\n      <td>2.500000</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Do a one hot encoding\ndf = pd.get_dummies(df,prefix='species')\nX = df[['sepal_length','sepal_width','petal_length','petal_width']]\nX = X.to_numpy()\n\ndf.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2024-03-05T04:51:19.715057Z","iopub.execute_input":"2024-03-05T04:51:19.715442Z","iopub.status.idle":"2024-03-05T04:51:19.756397Z","shell.execute_reply.started":"2024-03-05T04:51:19.715414Z","shell.execute_reply":"2024-03-05T04:51:19.755030Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"        sepal_length  sepal_width  petal_length  petal_width variety_setosa  \\\ncount     150.000000   150.000000    150.000000   150.000000            150   \nunique           NaN          NaN           NaN          NaN              2   \ntop              NaN          NaN           NaN          NaN          False   \nfreq             NaN          NaN           NaN          NaN            100   \nmean        5.843333     3.054000      3.758667     1.198667            NaN   \nstd         0.828066     0.433594      1.764420     0.763161            NaN   \nmin         4.300000     2.000000      1.000000     0.100000            NaN   \n25%         5.100000     2.800000      1.600000     0.300000            NaN   \n50%         5.800000     3.000000      4.350000     1.300000            NaN   \n75%         6.400000     3.300000      5.100000     1.800000            NaN   \nmax         7.900000     4.400000      6.900000     2.500000            NaN   \n\n       variety_versicolor variety_virginica  \ncount                 150               150  \nunique                  2                 2  \ntop                 False             False  \nfreq                  100               100  \nmean                  NaN               NaN  \nstd                   NaN               NaN  \nmin                   NaN               NaN  \n25%                   NaN               NaN  \n50%                   NaN               NaN  \n75%                   NaN               NaN  \nmax                   NaN               NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal_length</th>\n      <th>sepal_width</th>\n      <th>petal_length</th>\n      <th>petal_width</th>\n      <th>variety_setosa</th>\n      <th>variety_versicolor</th>\n      <th>variety_virginica</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>100</td>\n      <td>100</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.843333</td>\n      <td>3.054000</td>\n      <td>3.758667</td>\n      <td>1.198667</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.828066</td>\n      <td>0.433594</td>\n      <td>1.764420</td>\n      <td>0.763161</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4.300000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>0.100000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.100000</td>\n      <td>2.800000</td>\n      <td>1.600000</td>\n      <td>0.300000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.800000</td>\n      <td>3.000000</td>\n      <td>4.350000</td>\n      <td>1.300000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.400000</td>\n      <td>3.300000</td>\n      <td>5.100000</td>\n      <td>1.800000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.900000</td>\n      <td>4.400000</td>\n      <td>6.900000</td>\n      <td>2.500000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\ny = df.loc[:, 'variety_versicolor']\ny = y.to_numpy()\n\n# Shuffle the two dataset in unison\nperm = np.random.permutation(len(X))\nX = X[perm]\ny = y[perm]\n\nclf = Perceptron()\nclf.fit(X,y, num_iteration = 1000)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T04:52:21.651301Z","iopub.execute_input":"2024-03-05T04:52:21.651705Z","iopub.status.idle":"2024-03-05T04:52:21.697792Z","shell.execute_reply.started":"2024-03-05T04:52:21.651676Z","shell.execute_reply":"2024-03-05T04:52:21.696415Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Iteration 0 with error = 0.6666666666666666\nIteration 100 with error = 0.58\nIteration 200 with error = 0.5\nIteration 300 with error = 0.38666666666666666\nIteration 400 with error = 0.3466666666666667\nIteration 500 with error = 0.3933333333333333\nIteration 600 with error = 0.35333333333333333\nIteration 700 with error = 0.5666666666666667\nIteration 800 with error = 0.6333333333333333\nIteration 900 with error = 0.37333333333333335\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\n\nclass Perceptron():\n    '''\n        Perceptron Learning Algorithm that can be train using a \n        fit and predict methodology, without any library\n    '''\n    \n    def __init__(self):\n        self.weights = []\n        \n    def fit(self, X, y, learning_rate = 0.01, num_iteration = 100):\n        \n        num_row = len(X)\n        num_feature = len(X[0]) # Here we assume that we have a rectangular matrix\n        \n        # Randomly initalize the weights\n        for i in range(num_feature+1):\n            self.weights.append(random.uniform(0,1))\n        \n        # Launch the training algorithm\n        \n        for i in range(num_iteration):\n            \n            # Stochastic Gradient Descent\n            r_i = random.randint(0,num_row-1)\n            row = X[r_i]\n            yhat = self.predict(row)\n            error = (y[r_i] - yhat)\n            self.weights[0] = self.weights[0] + learning_rate*error\n\n            for f_i in range(num_feature):\n                self.weights[f_i] = self.weights[f_i] + learning_rate*error*row[f_i]\n                \n            if i % 100 == 0:\n                total_error = 0\n                for r_i in range(num_row):\n                    row = X[r_i]\n                    yhat = self.predict(row)\n                    error = (y[r_i] - yhat)\n                    total_error = total_error + error**2\n                mean_error = total_error/num_row\n                print(f\"Iteration {i} with error = {mean_error}\")\n        \n    def predict(self, row):\n            \n        # The activation start with the bias at weights == 0\n        activation = self.weights[0]\n        \n        # We iterate over the weights and the features in the given row\n        for weight, feature in zip(self.weights[1:], row):\n            activation = activation + weight*feature\n            \n        if activation >= 0.0:\n            return 1.0\n        return 0.0","metadata":{"execution":{"iopub.status.busy":"2024-03-05T04:52:36.337677Z","iopub.execute_input":"2024-03-05T04:52:36.338101Z","iopub.status.idle":"2024-03-05T04:52:36.355275Z","shell.execute_reply.started":"2024-03-05T04:52:36.338068Z","shell.execute_reply":"2024-03-05T04:52:36.353768Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import csv\n\ndef permute_together(X,y):\n    '''\n        Helper function to permute (shuffle) a matrix and a vector together\n    '''\n    \n    perm_X = []\n    perm_y = []\n    while len(X) != 0 and len(y) != 0:\n        \n        perm_id = random.randint(0,len(X)-1)\n        perm_X.append(X.pop(perm_id))\n        perm_y.append(y.pop(perm_id))\n        \n    return (perm_X, perm_y)\n\nclass DataFrame():\n    '''\n        Simple dataframe to mimick the pandas library\n    '''\n    def __init__(self):\n        self.header = [];\n        self.X = []\n        self.y = []\n        \n    def clean_string(self,string):\n        '''\n            Dummy function to clean up the iris dataset from (\")\n        '''\n        return string.replace('\"', '')\n    \n\n    def get_encoded_labels(self, target):\n        '''\n            Encode with 1 or 0 the y vector if it match our target variable\n        '''\n        labels = []\n        for label in self.y:\n            if label == target:\n                labels.append(1)\n            else:\n                labels.append(0)\n        return labels\n    \n    def read_csv(self, filename):\n        '''\n            Read the iris dataset CSV file and populate the header, the X and the y variables\n            needed for the perceptron\n        '''\n        with open(filename, newline='', encoding=\"utf-8-sig\") as csvfile:\n            csvreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n            \n            index = -1\n            for row in csvreader:\n                \n                # We have the header\n                if index == -1:\n                    self.header = [self.clean_string(s) for s in row[0].split(',')]\n                    index += 1\n                    continue\n                \n                # Here is the data\n                x = []\n                target = None\n                data = row[0].split(',')\n                for i in range(len(data)-1):\n                    x.append(float(data[i]))\n                \n                # Last item in the csv will be the target\n                self.y.append(self.clean_string(data[len(data)-1]))\n                self.X.append(x)\n                \n                index += 1\n        \n                \n\n# Data sets\ndf = DataFrame()\ndf.read_csv('/kaggle/input/iris-dataset/iris.csv')\n\nX = df.X\ny = df.get_encoded_labels('Versicolor') # encoding for 0 and 1\n\n# Shuffle the two dataset in unison\nX,y = permute_together(X,y)\n\nclf = Perceptron()\nclf.fit(X,y, num_iteration = 1000)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T04:53:16.717743Z","iopub.execute_input":"2024-03-05T04:53:16.718643Z","iopub.status.idle":"2024-03-05T04:53:16.763033Z","shell.execute_reply.started":"2024-03-05T04:53:16.718600Z","shell.execute_reply":"2024-03-05T04:53:16.761628Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Iteration 0 with error = 1.0\nIteration 100 with error = 0.0\nIteration 200 with error = 0.0\nIteration 300 with error = 0.0\nIteration 400 with error = 0.0\nIteration 500 with error = 0.0\nIteration 600 with error = 0.0\nIteration 700 with error = 0.0\nIteration 800 with error = 0.0\nIteration 900 with error = 0.0\n","output_type":"stream"}]}]}