{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-05T04:23:22.787249Z","iopub.execute_input":"2024-03-05T04:23:22.787658Z","iopub.status.idle":"2024-03-05T04:23:23.269395Z","shell.execute_reply.started":"2024-03-05T04:23:22.787620Z","shell.execute_reply":"2024-03-05T04:23:23.267857Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport random\n\n\n# Perceptron with numpy + pandas\n# The guy is great, will definetly give a shoutout\nclass Perceptron():\n    '''\n        Perceptron Learning Algorithm that can be train using a \n        fit and predict methodology with numpy\n    '''\n    \n    def __init__(self):\n        self.weights = []\n        \n    def fit(self, X, y, learning_rate = 0.01, num_iteration = 100):\n        \n        (num_row, num_feature) = X.shape\n        \n        # Randomly initalize the weights\n        self.weights = np.random.rand(num_feature+1) \n\n        # Launch the training algorithm\n        for i in range(num_iteration):\n            \n            # Stochastic Gradient Descent\n            r_i = random.randint(0,num_row-1)\n            row = X[r_i,:] # take the random sample from the dataset\n            yhat = self.predict(row)\n            error = (y[r_i] - yhat) # estimate of the gradient\n            self.weights[0] = self.weights[0] + learning_rate*error*1 # first weight one is the bias\n\n            # Update all parameters after bias\n            for f_i in range(num_feature):\n                self.weights[f_i] = self.weights[f_i] + learning_rate*error*row[f_i]\n                \n            if i % 100 == 0:\n                total_error = 0\n                for r_i in range(num_row):\n                    row = X[r_i,:]\n                    yhat = self.predict(row)\n                    error = (y[r_i] - yhat)\n                    total_error = total_error + error**2\n                mean_error = total_error/num_row\n                print(f\"Iteration {i} with error = {mean_error}\")\n        \n    def predict(self, row):\n            \n        # The activation start with the bias at weights == 0\n        activation = self.weights[0]\n        \n        # We iterate over the weights and the features in the given row\n        for weight, feature in zip(self.weights[1:], row):\n            activation = activation + weight*feature\n            \n        # Heaviside Step Function Activation\n        if activation >= 0.0:\n            return 1.0\n        return 0.0","metadata":{"execution":{"iopub.status.busy":"2024-03-05T04:23:43.454821Z","iopub.execute_input":"2024-03-05T04:23:43.455842Z","iopub.status.idle":"2024-03-05T04:23:43.470113Z","shell.execute_reply.started":"2024-03-05T04:23:43.455802Z","shell.execute_reply":"2024-03-05T04:23:43.468809Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Data sets\ndf = pd.read_csv('/home/yacine/Documents/rightbrain/data/iris_dataset.csv')\n\n# Do a one hot encoding\ndf = pd.get_dummies(df,prefix=['variety'])\nX = df[['sepal.length','sepal.width','petal.length','petal.width']]\nX = X.to_numpy()\ny = df['variety_Versicolor']\ny = y.to_numpy()\n\n# Shuffle the two dataset in unison\nperm = np.random.permutation(len(X))\nX = X[perm]\ny = y[perm]\n\nclf = Perceptron()\nclf.fit(X,y, num_iteration = 1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nclass Perceptron():\n    '''\n        Perceptron Learning Algorithm that can be train using a \n        fit and predict methodology, without any library\n    '''\n    \n    def __init__(self):\n        self.weights = []\n        \n    def fit(self, X, y, learning_rate = 0.01, num_iteration = 100):\n        \n        num_row = len(X)\n        num_feature = len(X[0]) # Here we assume that we have a rectangular matrix\n        \n        # Randomly initalize the weights\n        for i in range(num_feature+1):\n            self.weights.append(random.uniform(0,1))\n        \n        # Launch the training algorithm\n        \n        for i in range(num_iteration):\n            \n            # Stochastic Gradient Descent\n            r_i = random.randint(0,num_row-1)\n            row = X[r_i]\n            yhat = self.predict(row)\n            error = (y[r_i] - yhat)\n            self.weights[0] = self.weights[0] + learning_rate*error\n\n            for f_i in range(num_feature):\n                self.weights[f_i] = self.weights[f_i] + learning_rate*error*row[f_i]\n                \n            if i % 100 == 0:\n                total_error = 0\n                for r_i in range(num_row):\n                    row = X[r_i]\n                    yhat = self.predict(row)\n                    error = (y[r_i] - yhat)\n                    total_error = total_error + error**2\n                mean_error = total_error/num_row\n                print(f\"Iteration {i} with error = {mean_error}\")\n        \n    def predict(self, row):\n            \n        # The activation start with the bias at weights == 0\n        activation = self.weights[0]\n        \n        # We iterate over the weights and the features in the given row\n        for weight, feature in zip(self.weights[1:], row):\n            activation = activation + weight*feature\n            \n        if activation >= 0.0:\n            return 1.0\n        return 0.0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\ndef permute_together(X,y):\n    '''\n        Helper function to permute (shuffle) a matrix and a vector together\n    '''\n    \n    perm_X = []\n    perm_y = []\n    while len(X) != 0 and len(y) != 0:\n        \n        perm_id = random.randint(0,len(X)-1)\n        perm_X.append(X.pop(perm_id))\n        perm_y.append(y.pop(perm_id))\n        \n    return (perm_X, perm_y)\n\nclass DataFrame():\n    '''\n        Simple dataframe to mimick the pandas library\n    '''\n    def __init__(self):\n        self.header = [];\n        self.X = []\n        self.y = []\n        \n    def clean_string(self,string):\n        '''\n            Dummy function to clean up the iris dataset from (\")\n        '''\n        return string.replace('\"', '')\n    \n\n    def get_encoded_labels(self, target):\n        '''\n            Encode with 1 or 0 the y vector if it match our target variable\n        '''\n        labels = []\n        for label in self.y:\n            if label == target:\n                labels.append(1)\n            else:\n                labels.append(0)\n        return labels\n    \n    def read_csv(self, filename):\n        '''\n            Read the iris dataset CSV file and populate the header, the X and the y variables\n            needed for the perceptron\n        '''\n        with open(filename, newline='', encoding=\"utf-8-sig\") as csvfile:\n            csvreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n            \n            index = -1\n            for row in csvreader:\n                \n                # We have the header\n                if index == -1:\n                    self.header = [self.clean_string(s) for s in row[0].split(',')]\n                    index += 1\n                    continue\n                \n                # Here is the data\n                x = []\n                target = None\n                data = row[0].split(',')\n                for i in range(len(data)-1):\n                    x.append(float(data[i]))\n                \n                # Last item in the csv will be the target\n                self.y.append(self.clean_string(data[len(data)-1]))\n                self.X.append(x)\n                \n                index += 1\n        \n                \n\n# Data sets\ndf = DataFrame()\ndf.read_csv('/home/yacine/Documents/rightbrain/data/iris_dataset.csv')\n\nX = df.X\ny = df.get_encoded_labels('Versicolor') # encoding for 0 and 1\n\n# Shuffle the two dataset in unison\nX,y = permute_together(X,y)\n\nclf = Perceptron()\nclf.fit(X,y, num_iteration = 1000)","metadata":{},"execution_count":null,"outputs":[]}]}